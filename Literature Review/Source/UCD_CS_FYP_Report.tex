\documentclass[]{UCD_CS_FYP_Report}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsmath, amssymb, amsthm}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\lstset{
    mathescape=true,
    basicstyle = \ttfamily
}
%\usepackage[backend=biber,style=chicago-authordate]{biblatex}
\usepackage[backend=biber,style=nature]{biblatex}
\addbibresource{references.bib}


%%%%%%%%%%%%%%%%%%%%%%
%%% Input project details

\def\studentname{Sabeer Bakir}% Edit with your name
\def\studentid{16333886}% Edit with your student id
\def\projecttitle{Learning shortest path tours to all pubs in UK} % Edit with you project title
\def\supervisorname{Dr. Deepak Ajwani}


\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Content

\tableofcontents
%\pdfbookmark[0]{Table of Contents}{toc}\newpage
%\newpage


%%%%%%%%%%%%%%%%%%%%%%
%%% Your Abstract here
\abstract
Designing algorithms for NP-hard combinatorial optimization problems is a complex task. These problems are typically solved by using carefully designed heuristics and polynomial time approximation algorithms. However, these solutions require problem-specific knowledge and trial-and-error. Automating this process has been a long-held dream of AI researchers. In practical applications, generally the structure does not change within the same problem distribution. We can exploit this property to create new methods to find solutions to these optimization problems. In this thesis, we consider the Travelling Salesman Problem (TSP) and with the aid of a classification model, we aim to identify the edges that are contained in the TSP tour. 


%%%%%%%%
\chapter{Project Specification}
The problem is to find a shortest tour covering each of the 49,687 pub crawls in UK (a large instance of the classic Travelling Salesman Problem). Travelling Salesman Problem (TSP) is an NP-hard problem and even for 50 cities, the world's fastest supercomputer has no hope of going through the full count of tours one by one to pick out the shortest. However, the state-of-the-art heuristics have already solved this instance, to within 1 meter of accuracy (\url{http://www.math.uwaterloo.ca/tsp/uk/index.html}), in around 250 years of computer time.

The goal of this project is to explore, if it is possible to \textbf{learn} the shortest tour in a supervised manner. In other words, a classification model is required to be learnt that can predict the edges in the shortest TSP tour based on the features of edges. It is hoped that the learning approach would compute close-to-optimal solution in significantly less computation time.\\[0.5cm]

\LARGE Core
\normalsize
\begin{itemize}
    \item Identify features of edges that can discriminate between edges in the shortest TSP tour and edges that are not.
    \item Take random samples of the pub crawl dataset and use a state-of-the-art heuristic on that to get the ground truth for training.
    \item Use the identified features and the ground truth on random samples to train a classification model that learns the edges in the shortest TSP tour.
    \item Evaluate the accuracy of the classification model.
\end{itemize}
\LARGE Advanced
\normalsize
\begin{itemize}
    \item Compare the running time vs. optimality trade-off of the learning approach with the Concorde TSP solver (\url{http://www.math.uwaterloo.ca/tsp/concorde.html}).
    \item Improve the accuracy of the classification algorithm by careful feature selection.
\end{itemize}


%%%%%%%%
\chapter{Introduction}
The Travelling salesman problem (TSP) is a combinatorial optimization problem that is NP-hard and has caused considerable interest by theory and algorithm design communities in the past. In the realm of computational complexity, one of Karp's 21 NP-complete problems is the Hamiltonian Cycle Problem, this is a special case of the TSP.

The TSP was first introduced in the 1800s by the Irish mathematician Sir William Rowan Hamilton and the British mathematician Thomas Penyngton Kirkman. However, optimisation of the TSP only came about in the 1930s from Karl Menger in Vienna and Harvard. Menger defined the problem as, “Given a finite number of points, with known pairwise distances, find the shortest path connecting the points.” \cite{Menger}

Techniques that are used to solve these graph optimisation problems comes in three broad categories: exact algorithms, approximation algorithms and heuristics. Exact algorithms always find the optimal solution. It requires exponential time which does not scale well for large inputs. Approximation algorithms offer the desirable polynomial time solution but it is hard to guarantee the optimality for certain problems. Heuristics are typically fast but require problem specific research and experimenting from the algorithm designers.

Designing algorithms for NP-hard combinatorial optimization problems is a complex task. It has been shown with use of good heuristics and approximation algorithms that good estimates for solutions are possible. However, these solutions require problem-specific knowledge and trial-and-error. Is it possible to automate this learning process with the use of machine learning classification? In real world applications, the data is very rarely similar but the structures within the data do not differ drastically. In this thesis, we will explore a method to solve the TSP using classification to identify edges that are contained within the tour created by the TSP. We will see if it is possible to \emph{learn} the shortest path to all pubs in the UK. The applications of this can be applied in many areas such as, DNA Sequencing, Route Planning, Logistics Management, etc.

%%%%%%%%
\chapter{Related Work and Ideas}
\section{Background to Combinatorial Optimization}
Combinatorial optimization is an area of research that consists of finding an optimal ordering/subset from a finite set of objects. In many of these problems, exhaustive search is not an ideal option as these problems are NP-hard, meaning that there is no known polynomial time algorithm to solve them nonetheless there are exact algorithms \cite{Bellman:1962:DPT:321105.321111} for the TSP that are efficient for small input sizes. There is a demand for solutions to these problems in many domains such as network design, bioinformatics, game theory and many more \cite{combApplications} where the input sizes are much larger. Is it possible to still find good solutions to these problems? Due to the popularity of these problems, it has created a surge of algorithms that try to find a "good" solution close to the optimal solution. These methods include, approximation algorithms \cite{JohnMcGe97}, domain specific heuristics\cite{davidapplegate2007} and meta-heuristics \cite{Larranaga1999, NatureBasedHeuristics}. The TSP is no exception to these methods, the TSP is known as a classic problem in combinatorial optimization. 

\section{Exact Algorithms for the TSP}
Initially the travelling salesman problem was tackled using exact algorithms, searching for a complete solution. One of the first algorithms of this nature was the Held-Karp Algorithm \cite{Bellman:1962:DPT:321105.321111}. These exact algorithms searched the entire solution space and were often only useful for very small input sizes and typically is not suited towards real-world data with more nodes and edges. This is one of the earliest adoptions of \textit{Dynamic Programming}. The main disadvantage to this algorithm is its runtime complexity of $\mathcal{O}(N^22^N)$. Due to this, algorithm designers have looked towards other methods of optimization such as heuristics and approximation algorithms.

\section{Approximation Algorithms for the TSP}
Algorithm designers looked towards approximating the solution such that a close-to-optimal solution could be obtained in substantially less time than an exact algorithm. Many approximation algorithms have been applied to the TSP; one noteworthy method is Christofides \cite{JohnMcGe97}. There has been little to no improvement in approximation algorithms since. 

\textbf{Christofides:} ensures solutions that will be within a factor of 3/2 of the optimal solution. The tour is generated by firstly, constructing the minimum weight spanning tree; secondly, find the perfect matching on the induced subgraph with vertices of odd degree. Combining these two graphs will result in a connected multigraph in which every vertex will have even degree. A Hamiltonian Cycle is then formed on this multigraph. It's runtime complexity is larger than most heuristics due to perfect matching algorithm complexity.

\section{Heuristics for the TSP}
Designers explored domain specific heuristics to solve TSPs with some prior knowledge of the problem at hand. These methods are fast whilst yielding solutions very close-to-optimal. An example of such heuristics can be found in the Concorde TSP Solver \cite{davidapplegate2007}, a very powerful program used to solve TSPs with almost ideal solutions on graphs with tens of thousands of nodes.

\textbf{Constructive Heuristics:}
This approach differs from local search heuristics where a solution is given and optimality is obtained by making local changes at various points in the solution. Constructive heuristics begin with an empty solution and attempt local optimizations at each point in the solution. In \cite{JohnMcGe97}, algorithms such as \textit{Nearest Neighbour (NN)}, \textit{Greedy} and \textit{Clarke-Wright}, and are applied to the TSP. 
\begin{itemize}
  \item \textbf{Nearest Neighbour (NN): }This algorithm constructs a tour from starting at some arbitrary node and builds the cycle by adding the nearest neighbour from the current node. Its runtime complexity is $\mathcal{O}(N^2)$, this provides a decent solution. On average the path length is roughly 25\% longer than the optimal path.
  \item \textbf{Greedy: }Often mistaken for NN, it is constructed by taking the complete graph of the TSP instance, and it constructs the path starting with the shortest edge. It is further built by continuously adding the next shortest incident edge such that no node has degree greater than 2 or cycle length less than the total number of nodes. Its runtime complexity is slightly larger than that of NN at $\mathcal{O}(N^2logN)$ but the worst case solution is better than NN.
  \item \textbf{Clarke-Wright (CW): }This algorithm originally comes from a vehicle routing method designed by Clarke and Write \cite{Clarke:1964:SVC:2769344.2769349}. In terms of the TSP, an arbitrary city is chosen as a central point for which the salesman would return to, after each visit to another city. A \textit{savings} metric is defined as how much shorter the tour becomes if the salesman skips returning to the central point. A greedy approach is then utilized on these \textit{savings} values rather than the edge distances. Since this is a different approach of the greedy algorithm, its runtime complexity is similarly $\mathcal{O}(N^2logN)$. This method has better performance in comparison to the greedy algorithm, but it's worst case performance is the same.
\end{itemize}

\textbf{Concorde TSP Solver:}
In the 1990s, a collection of heuristics and functions have been designed by Applegate, Bixby, Chvátal, and Cook \cite{davidapplegate2007}. This solver holds the most records for solving large TSP instances with minuscule loss in accuracy. This precision does come at the cost of time, many large TSP problems take over 100-years of CPU time which isn't ideal for applications that require solutions swiftly. The applications of the Concorde include: gene-mapping \cite{10.1093/jhered/esg012}, protein function prediction \cite{Johnson2006}, vehicle routing \cite{ApplegateVPR}, etc. Nonetheless, according to \cite{MULDER2003827}, the Concorde “is widely regarded as the fastest TSP solver, for large instances, currently in existence.”

\newpage
\section{Meta-heuristics for the TSP}
Developing heuristics can be heavily time consuming. To remedy this learning process, meta-heuristics were implemented to augment algorithm designer’s ability to distinguish the details within the problem instances. They are a general technique for combinatorial optimization problems. Meta-heuristics have some operators designed for solving certain problems such as the TSP, in the case of genetic algorithms, we customize the mutation and crossover operators.

\textbf{Genetic algorithms:}
There have been many different attempts at solving the TSP with genetic algorithms. These methods rely heavily on mutation and crossover operators, these operators are trivial for some problems but for the TSP, a representation of the tour should be defined such that these operators can be developed. Most genetic algorithms designed for the TSP are using path representation where the cities are put into an ordered list where the first city in the list is the first city to be visited \cite{Larranaga1999}. Mutation is used to allow the algorithm to explore the solution space to avoid a local optimal solutions, whereas the crossover operator is used to increase the quality of solutions. Examples of these operators include \textit{Partially-mapped crossover} \cite{PartialCrossover} and  \textit{Exchange Mutation} \cite{ExchangeMutation}. In the partially-mapped crossover operator, a slice of one parent's string is mapped onto a slice of the other parent's string, whilst the remainder of the information is exchanged. In exchange mutation, two random cities are selected and are exchanged/swapped.

\textbf{Nature Based Heuristics:}
These types of heuristics apply concepts from nature to tackle complex computational problems. Examples of these heuristics include: Simulated Annealing, Evolutionary Algorithms, Ant Colony Optimization and Particle Swarm Optimization. Each of these methods share an underlying concept in nature and lend itself well to solving optimization problems. In a comparative study of nature inspired heuristics, on the TSP \cite{NatureBasedHeuristics}, it was found that an iterative local search (ILS) performed the best on most instances. The ILS algorithms performed surprisingly well on the TSP. 


%\section{Deep Learning techniques for the TSP}

\section{Machine Learning techniques for the TSP}
Because of the issues with the meta-heuristics, in recent years, researchers have started exploring if machine learning techniques can be used to \emph{learn} the TSP, and these techniques can be broadly categorised into reinforcement, unsupervised and supervised learning. 

\subsection{Reinforcement learning}
This method differs from supervised learning by not requiring input-output pairs of examples to train on. Instead, the algorithm aims to strike a balance between exploration of the solution space and exploiting previously learned knowledge about the solution space \cite{ReinforcementLearning}. Reinforcement learning is used as a natural framework for learning the evaluation function in \cite{DBLP:journals/corr/DaiKZDS17}. An \textit{off-policy} reinforcement learning algorithm such as \textit{Q}-Learning was utilized here which updates its rules on the \textit{Q}-Value rather than looking at past examples to learn. This technique was used over graph problems such as: Minimum Vertex Cut (MVC), Maximum Cut (MAXCUT), and the Travelling Salesman Problem (TSP). This type of approach lends itself to designing greedy heuristics for difficult combinatorial optimization problems. Heuristics as we know are commonly fast but in the area of optimization, require certain knowledge about the underlying problem. This approach attempts to build these heuristics whilst learning about the underlying problem.

\subsection{Unsupervised Machine Learning}
There has been study on solving the TSP using an unsupervised machine learning technique called “Hopfield Networks” \cite{unsupervisedML}. The network has $N^2$ neurons, where $N$ is the number of cities. These are arranged in a square formation with each neuron representing the order of which city is visited. The weights in this network allow this system to explore the solution space with the constraints of creating a valid tour and maintaining minimal distance. This method works well for small instances of the TSP but without careful modifications, this technique tends to not scale well to larger instances of the TSP.

\subsection{Supervised Machine Learning}
Supervised techniques mostly rely on deep learning techniques. Deep neural networks are powerful frameworks that can take large amounts of data and process it through a series of "hidden" layers, to extract features from the input and produce an output according to what it's trained to learn. In the case of the TSP, the input is the elements of the problem instance, the nodes and edges in graphs. It’s expected output is edges in the TSP that are part of the shortest tour. These neural networks automatically learn features relevant to the problem as well as the mapping from the input to the output. This mapping is only valid if the instance comes from the same distribution as was used for the training. However, using deep learning techniques suffers from many issues:
\begin{itemize}
    \item Training a deep neural network requires large amounts of training data. In order to acquire such training data, it would require solving NP-hard problems itself. The way to combat this is to train the network on small instances that are relatively easier to collect training data for, as patterns observed in smaller instances remain true for larger instances. 
    \item Deep learning frameworks suffer heavily from their lack of interpretability. It is important to be able to mathematically analyse and explain these methods. It is impractical to attempt to analyse an algorithm with thousands of parameters.
    \item Deep learning is not flexible either, adding new constraints or a change to the domain can have dramatic effects on the solution.
\end{itemize}

\textbf{Pointer Networks:} 
In a study on pointer networks to solve the TSP \cite{NIPS2015_5866}, they trained on smaller instances containing 5-20 points. They observed almost perfect results for 25 points, good results for 30 points and the accuracy decreased dramatically for 40 and above.

\textbf{Deep reinforcement learning:}
The use of an \textit{on-policy} technique using neural networks was applied in \cite{DeepRL} in a framework called \textsc{GCOMB}. Here they explore graph embedding techniques using \textit{Graph Convolutional Networks (GCN)} which are then fed into a neural network to learn a \textit{Q}-function in order to predict a solution. This solution would come in the form of an unordered or ordered set of nodes, depending on the problem. The advantages of this implementation include: 
\begin{itemize}
  \item \textbf{Scalability: }The proposed framework in the above study is able to scale to networks with millions of nodes and billions of edges.
  \item \textbf{Generalizability: }\textsc{GCOMB}'s framework allows itself to be applied to many combinatorial problems rather than focusing on one specific problem.
\end{itemize}
These advantages do come with the trade-off of interpretability as it is difficult to analyse such complex frameworks. We will attempt to remedy this issue in this thesis with our approach.

\subsection{Supervised Pruning}
There has been recent work in supervised pruning to solve combinatorial optimization problems in \cite{iAAA}. They have built a framework for reducing the search space of the problem by augmenting state-of-the-art solvers to solve the maximum clique problem. This method is different from the standard machine learning approaches where a tour is learnt. This approach identifies the vertices that are not part of the solution and prunes them from the data, such that the search space is reduced significantly. An approximation algorithm can then be used to find the maximum clique in a shorter time than if this algorithm was run on the full set of vertices.

Our approach will borrow techniques from \cite{iAAA} and apply it to the TSP.




%%%%%%%%
\chapter{Data Considerations}
The data is comprised of locations of pubs within the UK scraped from \url{https://www.pubsgalore.co.uk/}, a site containing addresses of all the pubs within the UK. The gathered data will include the name of each pub, a latitude  and longitude pair, and if the pub is open or closed.  Upon inspection there are roughly 70,000+ pubs. The data will be static and locally stored. We are using this as our primary source of data as it is regularly kept up-to-date and has very few missing values.

With each point, we will calculate a distance metric. For early training we will use Euclidean distance for each pair of points. These edges will require features to aid in the machine learning process, for this we will be paying close attention to related work methods to provide good characteristics for each edge.

We will be using the Concorde TSP Solver \cite{davidapplegate2007} to build a tour from our dataset. This will provide us with the edges that are in the solution set and will help create our target for each instance in the original data.
All the data used in this thesis will be made publicly available. Any data used in this project was fairly gathered and is not used in a malicious way.

There is a concern about the accuracy of the solution as we have opted to use Euclidean Distance measure between the pubs rather than the time taken to drive/walk or in some cases take a ferry. That information could be provided by the Google Maps API, but we have a limited amount of API calls which will not allow us to have a full set of new metrics.


%%%%%%%%
\chapter{Outline of Approach}
\section{Problem Statement}
In our case, lets assume for an instance of the TSP, it is represented as an undirected weighted graph $G = (V, E, w), w:E\to\mathbb{R}$, where each weight corresponds to the distance from each point. $V$ is the set of cities/points in the graph and $E$ is the set of edges/arcs that connect them. For each problem instance, let $G$ be a complete graph such that every pair of distinct vertices is joined by an edge.

The goal is to create a tour that traverses this graph, crossing every vertex in $V$ with minimal weight and return to beginning of the tour. Such a tour is known as a \textit{Hamiltonian cycle}.

\section{Approach}
We will attempt to use a simple binary classification model to learn whether an edge is part of the TSP tour or not. The advantage to this approach is the interpretability and scalability of this model over more complex frameworks such as deep learning. We will be able to analyse the parameters and see the key features that distinguish edges from each other.

We will be training several classifiers such as:
\begin{itemize}
    \item Random Forest: Each tree within the forest will be trained on different types of graph structures (i.e. big cities, islands, rural areas, etc.). Majority voting from these trees will calculate whether an edge is contained within the tour
    \item Naïve Bayes
    \item Linear Regression
\end{itemize}

The other classifiers will be used to compare the performance of the Random Forest algorithm.
Start with small subsets of the data (i.e. 1000 points), as the algorithm improves, we will increase the input size until we are able to fully classify the entire dataset.
We will prune the edges that are guaranteed not to be contained within the tour as in \cite{iAAA}. 

\newpage
\begin{figure}[h]
    \centering
  \includegraphics[width=0.5\linewidth]{Figures/Prune.pdf}
  \caption{Decision boundary created by classifier}
  \label{fig:Prune}
\end{figure}
In Fig \ref{fig:Prune}, these classifiers will create a decision boundary on the edges, where green edges are in the tour and red edges are not. Using repeated pruning, we can limit our data to edges that are part of the solution with few misclassified edges. After reducing the problem space, we can then execute an approximation algorithm on this smaller dataset, in turn providing close-to-optimal solution in significantly less time.


%%%%%%%%
\chapter{Project Workplan}
\begin{figure}[h]
    \centering
  \includegraphics[width=1\linewidth]{Figures/GanttChart.pdf}
  \caption{Project Workplan}
  \label{fig:Workplan}
\end{figure}

\begin{itemize}
    \item \textbf{Data Scraping: }
    
    Writing the script to collect the data.
    \item \textbf{Computation of Distance Metrics: }
    
    Calculating the distances for every pair of pubs.
    \item \textbf{Generating Ground Truth: }
    
    Finding a close-to-optimal solution through other methods to generate the \textit{target} column in the data. We will achieve this by running the Concorde TSP Solver.
    \item \textbf{Feature Engineering: }
    
    Finding features that will capture details in the edges to aid in the classification process.
    \item \textbf{Train Machine Learning Models/Evaluation and Testing: }
    
    Training, paramaterizing, evaluating and testing the machine learning models.
    \item \textbf{Write Up: }
    
    Experimental write up will be done in parallel starting at the same time as feature engineering.
    \item \textbf{Contingency: }
    
    We have allowed roughly one week for contingency for problems that will arise while carrying out this project. We believe this is an adequate amount of time to tackle the upcoming problems.
\end{itemize}

%%%%%%%%
\chapter{Summary and Conclusions}
Combinatorial optimization is an NP-Hard problem that has many applications. In this thesis, we consider the travelling salesman problem (TSP), one of the classical combinatorial optimization problems. We reviewed the various techniques to solve the TSP from exact algorithms to complex Deep Learning frameworks. We will use a machine learning classification framework to identify the edges in the TSP tour. This prediction is then used to prune edges that are not in a TSP tour. This enables us to scale-up exact and approximation algorithms. 



%%%%%%%%%%%%%%%%%%%%%%
%%%% Latex help.
%\input{latexhelp.tex}


%%%%%%%%%%%%%%%%%%%%%%
%%% Acknowledgements
\chapter*{Acknowledgements}
I would like to thank University College Dublin’s school of Computer Science for their support and Dr. Deepak Ajwani for his guidance throughout this project.


%%%% ADD YOUR BIBLIOGRAPHY HERE
\printbibliography

%%%%
%%%% maybe code listing here?

%%%%
\end{document}
%\end{article}